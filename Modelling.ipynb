{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3642aa41-1dc1-487a-88f2-f3e930778f49",
   "metadata": {},
   "source": [
    "# Modelling <a id=''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f2ff4-d87f-4513-a9e1-de0191a06307",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "  * 1 [Import libriaries and Load data](#load_data)\n",
    "  * 2 [Prepare data](#prepare_data)\n",
    "  * 3 [Overview of recommender systems](#overview)\n",
    "    * 3.1 [Two primary approaches](#approaches)\n",
    "    * 3.2 [Deep learning-based recommender systems](#DL_rec_systems)\n",
    "  * 4 [Modelling](#modelling)\n",
    "    * 4.1 [Item2Vec and hierarchical Item2Vec models](#item2vec)\n",
    "    * 4.2 [Hyperparameters](#hyperparameters)\n",
    "      * 4.2.1 [Regularization parameter $\\lambda_{cat}$](#lambda_cat)\n",
    "      * 4.2.2 [Embedding dimension $dim_{embed}$](#dim_embedding)\n",
    "      * 4.2.3 [Threshold for item frequency $f_{thresh}$](#f_tresh)\n",
    "      * 4.2.4 [Learning rate and batch size](#learning_rate)\n",
    "      * 4.2.5 [Loss function](#loss_function)\n",
    "      * 4.2.6 [Similarity measure](#similarity_measure)\n",
    "  * 5 [Evaluation](#evaluation)\n",
    "  * 6 [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50d1d0-2333-486f-809e-9cc577ecea58",
   "metadata": {},
   "source": [
    "## 1 Import libraries and Load data<a id='load_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091ffc2a-7e7c-4719-bf1d-671969baa47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.2.0\n",
      "numpy version:  1.25.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from itertools import permutations, chain\n",
    "\n",
    "from HuffmanTree import HuffmanNode,build_huffman_tree,generate_codebook,visualize_huffman_tree\n",
    "from CategoryTree import TreeNode, build_tree, add_to_node, build_category_tree, get_path\n",
    "from parameters import Params\n",
    "from ItemMap import ItemMap\n",
    "from HierarchicalItem2Vec import HierarchicalItem2Vec, Trainer\n",
    "from batch_tool import BatchToolItem # this is obsolete as of Sep 25, 2025\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print('torch version: ', torch.__version__)\n",
    "print('numpy version: ', np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b717f2-b341-4986-99c0-835140b689fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evt= pd.read_csv('events_df.csv')\n",
    "df_cat= pd.read_csv('category_df.csv')\n",
    "\n",
    "df_trs = df_evt[(df_evt['event'] == 'transaction') & (df_evt['categoryid']> -1)]\n",
    "df_freq = df_trs.groupby('itemid').agg(frequency = pd.NamedAgg(column='itemid', aggfunc='size'),\n",
    "                                      categoryid = pd.NamedAgg(column='categoryid',aggfunc= 'first')).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef436a-adbe-419a-85d3-952aecae2555",
   "metadata": {},
   "source": [
    "## 2 Prepare data<a id='prepare_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a95b90b-75ca-4834-aa90-76607634a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visitorid         13027\n",
      "session_by_day    13027\n",
      "items             13027\n",
      "dtype: int64\n",
      "visitorid         3055\n",
      "session_by_day    3055\n",
      "items             3055\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_items = df_trs.groupby(['visitorid','session_by_day']).agg(items = pd.NamedAgg(column='itemid', aggfunc=list)).reset_index()\n",
    "print(df_items.count())\n",
    "\n",
    "df_filtered = df_items[df_items['items'].apply(len) > 1]\n",
    "print(df_filtered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388dad56-5b39-4d7b-8aaa-3000d51b43aa",
   "metadata": {},
   "source": [
    "**To optimize model performance, we apply data pruning to remove low-frequency or unused items. Such items can degrade the quality of learned embeddings by introducing noise and distorting the structure of shared internal nodes in the Huffman tree, which plays a key role in efficient hierarchical softmax computation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17552961-efc3-40e2-81c8-04fe417cf387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemid        11645\n",
      "frequency     11645\n",
      "categoryid    11645\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "itemid        7547\n",
       "frequency     7547\n",
       "categoryid    7547\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = df_filtered['items'].tolist()\n",
    "\n",
    "flat = list(chain.from_iterable(items))\n",
    "set_items = set(flat)\n",
    "\n",
    "print(df_freq.count())\n",
    "df_freq = df_freq.loc[df_freq['itemid'].isin(set_items)]\n",
    "df_freq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7421183-017c-4223-a165-a686ca0d779a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frequency\n",
       "1      4025\n",
       "2      1705\n",
       "3       780\n",
       "4       399\n",
       "5       222\n",
       "6       114\n",
       "7        85\n",
       "8        46\n",
       "9        40\n",
       "10       25\n",
       "11       25\n",
       "12       17\n",
       "13        9\n",
       "14        8\n",
       "19        6\n",
       "15        5\n",
       "16        5\n",
       "20        3\n",
       "27        2\n",
       "38        2\n",
       "18        2\n",
       "17        2\n",
       "31        2\n",
       "25        2\n",
       "46        2\n",
       "23        2\n",
       "97        1\n",
       "41        1\n",
       "92        1\n",
       "37        1\n",
       "29        1\n",
       "33        1\n",
       "28        1\n",
       "35        1\n",
       "32        1\n",
       "45        1\n",
       "22        1\n",
       "133       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freq['frequency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdfa144d-72bb-48bf-9b8e-5907f3a423d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = [461686,119736,213834,7943,312728] # highest frequency items\n",
    "do_test = False\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "for items_in_session in items:\n",
    "    \n",
    "    pairs = list(permutations(items_in_session, 2))\n",
    "    for pair in pairs:\n",
    "        target_id, context_id = pair[0],pair[1]\n",
    "        if target_id not in test_items and context_id not in test_items and do_test:\n",
    "            continue\n",
    "        inputs.append([target_id])\n",
    "        outputs.append(context_id)\n",
    "        \n",
    "# Convert to torch.tensor\n",
    "X = torch.tensor(inputs, dtype=torch.long)\n",
    "y = torch.tensor(outputs, dtype=torch.long)\n",
    "\n",
    "# Split using train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Wrap in TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd5f43-3578-44d9-b4f0-c0ea4ec2a30f",
   "metadata": {},
   "source": [
    "## 3 Overview of Recommender Systems<a id='overview'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caacacb-d4ea-457d-ae40-30a5db8ea0c1",
   "metadata": {},
   "source": [
    "### 3.1 Two primary approaches<a id='approaches'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21cfddd-09bb-488e-b3c0-c26c5b3e7b5d",
   "metadata": {},
   "source": [
    "*Recommender systems* suggest items to users based on their preferences, behavior, or similarities with others. They're widely used in platforms like Netflix, Amazon, Spotify, and YouTube. The primary goal is to predict a userâ€™s preferences and recommend items they are likely to engage with such as movies, products, news articles, or even people. There are mainly two core types of recommender systems, **collaborative filtering** and **content-based filtering**. \n",
    "\n",
    "**Collaborative filtering** can be further divided into:\n",
    "* *User-based filtering*, which identifies users with similar preferences and recommends items those users have liked.\n",
    "* *Item-based filtering*, which finds items similar to those the user has previously liked and recommends them.\n",
    "\n",
    "Similarity between users or items is typically computed using distance or similarity measures such as *Euclidean distance, Pearson correlation*, or *cosine similarity*. It's important to note that in collaborative filtering, item similarity is not based on the inherent features of the items, but rather on user interaction patterns. Item-based collaborative filtering is particularly effective in systems where the number of users significantly exceeds the number of items. \n",
    "However, collaborative filtering has several limitations:\n",
    "\n",
    "- Data sparsity: User-item interaction matrices are often sparse, making it difficult to identify meaningful patterns.\n",
    "- Cold start: The system struggles to make recommendations for new users or new items due to a lack of interaction data.\n",
    "- Scalability: Performance can degrade as the number of users or items grows.\n",
    "- Popularity bias: Tends to over-recommend popular items, reducing personalization and diversity.\n",
    "- Lack of Interpretability: Recommendations are based on patterns in user behavior, not explicit item attributes.\n",
    "- Gray sheep problem: Users with unique or atypical preferences may receive poor recommendations.\n",
    "\n",
    "**Content-based filtering**, on the other hand, recommends items that are similar to those a user has liked in the past. These similarities are determined based on item attributes or features, such as categories, tags, genres, or other metadata. This approach can help address some of the limitations of collaborative filtering.\n",
    "\n",
    "- No Need for Other Users' Data: Recommendations are based solely on the userâ€™s own preferences and item features.\n",
    "- Handles Cold Start (User Side): Can recommend items to new users after only a few interactions.\n",
    "- Interpretable Recommendations.\n",
    "- Less prone to recommending only popular items.\n",
    "- Privacy-Friendly: Doesnâ€™t require analyzing other usersâ€™ data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181355d-6104-4f6f-b1ca-5293756285e5",
   "metadata": {},
   "source": [
    "### 3.2 Deep learning-based recommender systems<a id='DL_rec_systems'></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e3d09-90ac-4ebd-996e-19f4012aca35",
   "metadata": {},
   "source": [
    "\n",
    "In recent years, deep learning-based recommender systems have gained prominence due to their superior performance and ability to model complex, non-linear relationships between users and items. The following points summarize their key advantages over traditional methods:\n",
    "\n",
    "- Better at Capturing Non-Linear and Complex Patterns\n",
    "- Better at Cold Start and Sparse Data\n",
    "- Effective Feature Representation (Embeddings): can automatically learn *dense, low-dimensional representations (embeddings)* of users and items from sparse interaction data\n",
    "- Integration of Multiple Data Types (Multimodal Inputs)\n",
    "- Personalized and Context-Aware Recommendations: Deep models can learn user-specific behaviors, preferences, and temporal patterns.\n",
    "\n",
    "Deep learning models excel at capturing high-level patterns and personalization through architectures such as deep neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), and attention-based mechanisms. As a result, they often outperform traditional approaches, particularly in large-scale and dynamic recommendation environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67552792-e123-4b2b-a09b-5aecdfda8355",
   "metadata": {},
   "source": [
    "## 4 Modelling <a id='modelling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e949c0d-52b0-404f-ae2f-f55fa301aba1",
   "metadata": {},
   "source": [
    "### 4.1 Item2Vec and hierarchical Item2Vec models<a id='item2vec'></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e966eda-b0b7-4d4a-9170-3b9382dc88c1",
   "metadata": {},
   "source": [
    "This project focuses on the Item2Vec model, which is based on the Word2Vec architecture originally developed for learning optimized word embeddings in natural language processing (NLP). By drawing an analogy between sequences of words and sequences of user-item interactions, Item2Vec learns dense vector representations of items that capture similarity and co-occurrence patterns. This results in more effective recommendations through meaningful item embeddings.\n",
    "\n",
    "Item2Vec is an item-based collaborative filtering technique and, as such, inherits some of the limitations discussed in the previous section, such as cold-start problems and a lack of content awareness. To address these issues, we explore a hybrid approach called Hierarchical Item2Vec, which integrates hierarchical content-based information into the embedding process. This method helps mitigate the shortcomings of purely collaborative approaches by incorporating category-level knowledge.\n",
    "\n",
    "Next, we will explore and fine-tune the model architecture and its components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f7b6f-b124-41ef-b2dc-fa610cc18c1a",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameters<a id='hyperparameters'></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925877d-f39b-442d-af9d-f38bf7107a62",
   "metadata": {},
   "source": [
    "To optimize model performance, we consider the following **hyperparameters**:\n",
    "\n",
    "\n",
    "* $\\lambda_{cat}$ \n",
    "* Embedding dimension $dim_{embed}$ \n",
    "* Threshold for item frequency $f_{thresh}$\n",
    "* Learning rate \n",
    "* Loss function = {Negative sampling, Hierarchical softmax}\n",
    "* Similarity measure: cosine similarity, distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089afaa-598f-4f7f-8a78-b481be542f17",
   "metadata": {},
   "source": [
    "#### 4.2.1 Regularization parameter $\\lambda_{cat}$<a id='lambda_cat'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d0223-42c0-4e7c-ba52-0f0f6b0e47dd",
   "metadata": {},
   "source": [
    "The value of $\\lambda_{cat}$ regulates the strength of alignment between item embeddings and their respective category embeddings.\n",
    "\n",
    "We will select the following values during hyperparameter tuning:: \n",
    "\n",
    "<span style=\"color:red\"> $\\lambda_{cat}$ = {0, 0.1, 1, 10} </span>\n",
    "\n",
    "| $\\lambda_{\\text{cat}}$ Value                 | Meaning / Effect                                                                 | When to Use                                                           |\n",
    "| -------------------------------------------- | -------------------------------------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **0**                                        | No category alignment at all. Equivalent to Item2Vec.            | Baseline comparison; when category data is noisy or not useful.       |\n",
    "| **1 $\\times 10^{-4}$ to 1 $\\times 10^{-3}$** | Very weak alignment. Minor influence from category embeddings.                   | Categories are somewhat useful, but item-level patterns dominate.     |\n",
    "| **1 $\\times 10^{-2}$ to 0.1**              | Moderate alignment. Balanced influence between item behavior and category info.  | Often a good starting point for tuning; works well in many scenarios. |\n",
    "| **0.1 to 1.0**                           | Strong alignment. Item embeddings are pulled significantly toward category ones. | When categories are well-defined and strongly predictive.             |\n",
    "| **>1.0**                                   | Very strong alignment. Item embeddings may lose individuality.                   | Only use if category structure is known to be highly reliable.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc09605-06b7-4a94-9692-7b2a75de69c3",
   "metadata": {},
   "source": [
    "#### 4.2.2 Embedding dimension $dim_{embed}$  <a id='dim_embedding'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045edc6-8915-4b3d-81b9-a10e789ad32a",
   "metadata": {},
   "source": [
    "The optimal embedding dimension depends on both the number of items and the underlying characteristics of the data. In our case, the dataset contains approximately 7,000 items suitable for training. Given this scale, we experiment with \n",
    "\n",
    "<span style=\"color:red\"> embedding dimensions $dim_{embed}$ = {32, 64, 128}</span>. \n",
    "\n",
    "These values strike a balance between model capacity and generalization: smaller dimensions may not capture enough semantic information, while larger dimensions risk overfitting and increased computational cost. Through experiments, we aim to identify the dimension that provides the best trade-off between performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037aff2-f05b-40af-8dc2-a3431f082fd4",
   "metadata": {},
   "source": [
    "#### 4.2.3 Threshold for item frequency $f_{thresh}$ <a id='f_tresh'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e48aa-d80e-410b-8958-4f7848799511",
   "metadata": {},
   "source": [
    "#### 4.2.4 Learning rate and batch size<a id='learning_rate'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebc000-0aed-4821-a7a2-d83dc35b58f4",
   "metadata": {},
   "source": [
    "Learning rate and batch size are critical hyperparameters that significantly influence model convergence and overall performance. A smaller learning rate may lead to more stable convergence but slower training, while a larger batch size can improve training efficiency but may reduce generalization.\n",
    "\n",
    "<span style=\"color:red\"> Learning rate = {0.005, 0.01, 0.05} </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb386e6-3a97-4269-8e21-49169ece99fd",
   "metadata": {},
   "source": [
    "#### 4.2.5 Loss function <a id='loss_function'></a> \n",
    "\n",
    "The **Negative Sampling** method updates only a small number of negative samples along with the true target, significantly reducing computational cost. While it is fast, simple, and easily parallelizable, it relies on a stochastic approximation of the softmax function. As a result, it is sensitive to the choice of sampling strategy and does not produce outputs with a clear probabilistic interpretation. It is particularly suitable for models with a large output space (e.g., over 10,000 items), where computing the full softmax would be computationally prohibitive.\n",
    "\n",
    "The **Hierarchical Softmax** method\n",
    "\n",
    "Considering our training data contains fewer than 10K items, and part of our focus is on rare items and their structure, hierarchical softmax is a more effective choice due to its ability to leverage shared paths in the tree, improving representation for infrequent items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926e629-f32c-4d14-8cc3-22d059598941",
   "metadata": {},
   "source": [
    "#### 4.2.6 Similarity measure <a id='similarity_measure'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcc09a-abfb-4d0d-bc0f-042bc2dd845e",
   "metadata": {},
   "source": [
    "Since our model learns embeddings that encode semantic relationships between items, **cosine similarity** is the most suitable similarity measure for our study. It effectively captures the orientation (rather than magnitude) of embedding vectors. This is particularly important in high-dimensional spaces, where the angle between vectors provides a more meaningful measure of similarity than Euclidean distance. This makes it especially well-suited for tasks such as *ranking, retrieval, and recommendation*.\n",
    "\n",
    "Furthermore, cosine similarity can be used to help determine the optimal embedding dimension. If the dimension is too small, item vectors tend to cluster tightly together, resulting in uniformly high cosine similarity even before training, which limits the model's ability to distinguish between items. A good strategy is to choose an embedding dimension large enough so that cosine similarities between random item pairs are below **0.4** *before* training, while ensuring that co-occurring items exhibit high similarity, typically above **0.8**, *after* training.\n",
    "\n",
    "| Cosine Similarity Score | Interpretation                                          |\n",
    "| ----------------------- | ------------------------------------------------------- |\n",
    "| **> 0.8**               | Very high similarity (strong recommendation candidates) |\n",
    "| **0.6 â€“ 0.8**           | High similarity (related items, likely of interest)     |\n",
    "| **0.4 â€“ 0.6**           | Moderate similarity (some shared context)               |\n",
    "| **< 0.4**               | Low or weak similarity                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2858b5-f3dc-43b4-9979-a0d5180dc3f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca9c959-2b4b-427a-b9a6-61c08bbc7b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of items  7547  total number of categories  1670\n",
      "Total number of inner nodes : 7546\n"
     ]
    }
   ],
   "source": [
    "# Huffman Tree\n",
    "begin_index = 500000\n",
    "imap = ItemMap(df_freq, df_cat)\n",
    "itemmap = imap.dict_items\n",
    "flat_itemmap = imap.flat_items\n",
    "total_inner_nodes, huff_tree = build_huffman_tree(begin_index, None, flat_itemmap, None)\n",
    "print(f'Total number of inner nodes : {total_inner_nodes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d1ae57-5142-4712-9fd5-674e9267328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category Tree\n",
    "root_code = 10000\n",
    "cat_tree =  build_category_tree(root_code, df_cat, itemmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05a2aca-4c55-4060-9b21-105675a8258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()\n",
    "params.model_name = 'HierarchicalItem2Vec'\n",
    "params.model_dir = \"weights/{}\".format(params.model_name)\n",
    "params.dim_embedding = 64\n",
    "params.lambda_cat = 0.1\n",
    "params.batch_size = 4\n",
    "params.n_epochs = 5 # 1 for test\n",
    "os.makedirs(params.model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a476f97-e00e-4ba7-860d-a384d9fad7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchtool = BatchToolItem(imap, params)\n",
    "hi2v = HierarchicalItem2Vec(imap, params, huff_tree, cat_tree)\n",
    "optimizer = torch.optim.Adam(params = hi2v.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3104bdb7-3ffc-47cd-a10a-092646b9251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n",
      "461686 || 439586 (0.442) 72521 (0.435) 166327 (0.435) 116677 (0.409) \n",
      "\n",
      "119736 || 268955 (0.410) 4289 (0.407) 324759 (0.406) 15 (0.399) \n",
      "\n",
      "213834 || 465606 (0.401) 162722 (0.396) 190000 (0.387) 454856 (0.379) \n",
      "\n",
      "7943 || 148478 (0.394) 253615 (0.383) 130599 (0.378) 128513 (0.377) \n",
      "\n",
      "312728 || 253500 (0.397) 311931 (0.397) 343377 (0.394) 446411 (0.383) \n",
      "\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15749/15749 [02:59<00:00, 87.73it/s, loss=0.755]\n",
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3938/3938 [00:27<00:00, 144.54it/s, loss=0.623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "     Train Loss: 0.69\n",
      "     Valid Loss: 0.6\n",
      "     Training Time (mins): 3.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15749/15749 [03:17<00:00, 79.92it/s, loss=0.687]\n",
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3938/3938 [00:32<00:00, 121.72it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5\n",
      "     Train Loss: 0.51\n",
      "     Valid Loss: 0.57\n",
      "     Training Time (mins): 3.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15749/15749 [03:20<00:00, 78.64it/s, loss=0.449]\n",
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3938/3938 [00:32<00:00, 120.68it/s, loss=0.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5\n",
      "     Train Loss: 0.45\n",
      "     Valid Loss: 0.58\n",
      "     Training Time (mins): 3.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15749/15749 [03:19<00:00, 78.86it/s, loss=0.402]\n",
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3938/3938 [00:32<00:00, 122.21it/s, loss=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5\n",
      "     Train Loss: 0.42\n",
      "     Valid Loss: 0.59\n",
      "     Training Time (mins): 3.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15749/15749 [03:21<00:00, 78.07it/s, loss=0.692]\n",
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3938/3938 [00:33<00:00, 116.78it/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5\n",
      "     Train Loss: 0.41\n",
      "     Valid Loss: 0.62\n",
      "     Training Time (mins): 3.4\n",
      "\n",
      "\n",
      "-----------\n",
      "461686 || 171878 (0.692) 113712 (0.679) 442300 (0.672) 10572 (0.657) \n",
      "\n",
      "119736 || 236949 (0.713) 309273 (0.711) 377133 (0.708) 338660 (0.707) \n",
      "\n",
      "213834 || 445351 (0.731) 290146 (0.724) 48141 (0.718) 90240 (0.712) \n",
      "\n",
      "7943 || 65540 (0.608) 133332 (0.605) 130724 (0.605) 432011 (0.593) \n",
      "\n",
      "312728 || 388119 (0.594) 98899 (0.566) 123664 (0.549) 64820 (0.548) \n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=hi2v,\n",
    "        params=params,\n",
    "        optimizer=optimizer,\n",
    "        train_iter=train_dataset,\n",
    "        valid_iter=val_dataset,\n",
    "        map=imap,\n",
    "        method =batchtool,\n",
    "        debug = 0\n",
    "    )\n",
    "trainer.test_tokens = [461686,119736,213834,7943,312728]\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd805b-61ee-4199-a385-b9e0a4f8040a",
   "metadata": {},
   "source": [
    "## 5 Evaluation <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5085d33-4e9e-41bc-8e3a-87a018aa0a0b",
   "metadata": {},
   "source": [
    "Test item groups: high-frequency, moderate-frequency items, cold items.\n",
    "For problems at hand, ranking-based evaluation metrics such as Precision@k, Recall@k, NDCG, MAP are most appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf57ba8-ee5e-4f64-b163-7c9a46eeb758",
   "metadata": {},
   "source": [
    "* Precision at K\n",
    "* Recall at K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdaa83-75c3-4f5f-a0a2-fb34060712db",
   "metadata": {},
   "source": [
    "## 6 Conclusion <a id='conclusion'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bef0a-cb5f-4476-a748-1e17d605b5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
